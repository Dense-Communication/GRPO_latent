#!/usr/bin/env python3
"""
å¿«é€Ÿå¥å…¨æ€§æ£€æŸ¥è„šæœ¬ - 5åˆ†é’Ÿå†…éªŒè¯è®­ç»ƒç®¡é“æ˜¯å¦æ­£å¸¸

æ£€æŸ¥é¡¹ç›®ï¼š
1. æ•°æ®åŠ è½½æ˜¯å¦æ­£ç¡®
2. æ¨¡å‹èƒ½å¦æ­£å¸¸ç”Ÿæˆ
3. Reward å‡½æ•°æ˜¯å¦èƒ½ç»™å‡ºæ­£ç¡®åˆ†æ•°
4. Prompt æ˜¯å¦æ­£ç¡®ä¼ é€’
"""

import os
import sys

# è®¾ç½®ç¦»çº¿æ¨¡å¼
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_HUB_OFFLINE"] = "1"
os.environ["HF_DATASETS_OFFLINE"] = "1"

import torch
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/p/scratch/westai0052/liu52/verl-agent')
sys.path.insert(0, '/p/scratch/westai0052/liu52/verl-agent/test_script')

from custom_gsm8k_reward import compute_score, extract_answer

def print_header(title):
    print("\n" + "=" * 60)
    print(f"  {title}")
    print("=" * 60)

def print_result(name, passed, details=""):
    status = "âœ… PASS" if passed else "âŒ FAIL"
    print(f"{status} | {name}")
    if details:
        print(f"       {details}")

def main():
    print_header("GSM8K GRPO è®­ç»ƒç®¡é“å¥å…¨æ€§æ£€æŸ¥")

    model_path = "/p/scratch/westai0052/liu52/models/Qwen2.5-1.5B-Instruct"
    data_path = "/p/scratch/westai0052/liu52/verl-agent/test_script/data/train.parquet"

    all_passed = True

    # ============ æ£€æŸ¥ 1: æ•°æ®åŠ è½½ ============
    print_header("1. æ•°æ®åŠ è½½æ£€æŸ¥")
    try:
        df = pd.read_parquet(data_path)
        print_result("è¯»å– parquet æ–‡ä»¶", True, f"å…± {len(df)} æ¡æ•°æ®")

        # æ£€æŸ¥å¿…éœ€çš„åˆ—
        required_cols = ['data_source', 'prompt', 'reward_model']
        missing = [c for c in required_cols if c not in df.columns]
        if missing:
            print_result("å¿…éœ€åˆ—å­˜åœ¨", False, f"ç¼ºå°‘: {missing}")
            all_passed = False
        else:
            print_result("å¿…éœ€åˆ—å­˜åœ¨", True)

        # æ£€æŸ¥ prompt æ ¼å¼
        sample_prompt = df.iloc[0]['prompt']
        is_numpy = isinstance(sample_prompt, np.ndarray)
        has_content = len(sample_prompt) > 0 and 'content' in sample_prompt[0]
        print_result("Prompt æ ¼å¼æ­£ç¡®", has_content,
                    f"ç±»å‹: {type(sample_prompt).__name__}, åŒ…å« content: {has_content}")

        # æ£€æŸ¥ ground_truth
        gt = df.iloc[0]['reward_model']['ground_truth']
        print_result("Ground truth å­˜åœ¨", gt is not None, f"ç¤ºä¾‹: '{gt}'")

    except Exception as e:
        print_result("æ•°æ®åŠ è½½", False, str(e))
        all_passed = False

    # ============ æ£€æŸ¥ 2: Tokenizer å’Œ Chat Template ============
    print_header("2. Tokenizer æ£€æŸ¥")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        print_result("åŠ è½½ tokenizer", True)

        # æµ‹è¯• chat template
        sample_prompt = df.iloc[0]['prompt']
        if isinstance(sample_prompt, np.ndarray):
            sample_prompt = sample_prompt.tolist()

        chat_text = tokenizer.apply_chat_template(
            sample_prompt,
            add_generation_prompt=True,
            tokenize=False
        )

        # éªŒè¯é—®é¢˜å†…å®¹åœ¨ chat ä¸­
        question_snippet = sample_prompt[0]['content'][:50]
        content_in_chat = question_snippet in chat_text
        print_result("Chat template åŒ…å«é—®é¢˜å†…å®¹", content_in_chat,
                    f"é—®é¢˜ç‰‡æ®µ: '{question_snippet}...'")

        if not content_in_chat:
            all_passed = False

        tokens = tokenizer.encode(chat_text, add_special_tokens=False)
        print_result("Tokenization", True, f"ç”Ÿæˆ {len(tokens)} ä¸ª tokens")

    except Exception as e:
        print_result("Tokenizer æ£€æŸ¥", False, str(e))
        all_passed = False

    # ============ æ£€æŸ¥ 3: æ¨¡å‹ç”Ÿæˆ ============
    print_header("3. æ¨¡å‹ç”Ÿæˆæ£€æŸ¥")
    try:
        print("åŠ è½½æ¨¡å‹ä¸­... (è¿™å¯èƒ½éœ€è¦ 30 ç§’)")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        print_result("åŠ è½½æ¨¡å‹", True)

        # ç”Ÿæˆå›å¤
        inputs = tokenizer(chat_text, return_tensors="pt").to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )

        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        print_result("ç”Ÿæˆå›å¤", len(response) > 10, f"ç”Ÿæˆ {len(response)} å­—ç¬¦")

        print("\n--- æ¨¡å‹ç”Ÿæˆç¤ºä¾‹ ---")
        print(f"é—®é¢˜: {sample_prompt[0]['content'][:100]}...")
        print(f"å›å¤: {response[:300]}...")

        # æ£€æŸ¥å›å¤æ˜¯å¦æ˜¯ä¹±ç 
        # ç®€å•æ£€æŸ¥ï¼šä¹±ç é€šå¸¸åŒ…å«å¤§é‡é ASCII æ··åˆå­—ç¬¦
        ascii_ratio = sum(1 for c in response if ord(c) < 128) / max(len(response), 1)
        is_coherent = ascii_ratio > 0.5 or response.count('####') > 0
        print_result("å›å¤å†…å®¹æœ‰æ„ä¹‰", is_coherent, f"ASCIIæ¯”ä¾‹: {ascii_ratio:.2%}")

        if not is_coherent:
            all_passed = False

    except Exception as e:
        print_result("æ¨¡å‹ç”Ÿæˆ", False, str(e))
        all_passed = False

    # ============ æ£€æŸ¥ 4: Reward å‡½æ•° ============
    print_header("4. Reward å‡½æ•°æ£€æŸ¥")
    try:
        # æµ‹è¯•æ­£ç¡®ç­”æ¡ˆ
        test_response_correct = "Let me calculate step by step.\n48/2 = 24\n48 + 24 = 72\n#### 72"
        gt = "72"
        score_correct = compute_score("openai/gsm8k", test_response_correct, gt)
        print_result("æ­£ç¡®ç­”æ¡ˆå¾—åˆ†", score_correct == 1.0, f"å¾—åˆ†: {score_correct}")

        # æµ‹è¯•é”™è¯¯ç­”æ¡ˆ
        test_response_wrong = "The answer is #### 50"
        score_wrong = compute_score("openai/gsm8k", test_response_wrong, gt)
        print_result("é”™è¯¯ç­”æ¡ˆå¾—åˆ†", score_wrong == 0.0, f"å¾—åˆ†: {score_wrong}")

        # æµ‹è¯•æ— æ ¼å¼ç­”æ¡ˆ
        test_response_no_format = "The answer is 72."
        score_no_format = compute_score("openai/gsm8k", test_response_no_format, gt)
        print_result("æ— æ ¼å¼ç­”æ¡ˆå¾—åˆ†", score_no_format == 0, f"å¾—åˆ†: {score_no_format}")

        # æµ‹è¯•æ¨¡å‹å®é™…ç”Ÿæˆçš„å›å¤
        if 'response' in dir():
            answer = extract_answer(response, method="strict")
            actual_gt = df.iloc[0]['reward_model']['ground_truth']
            actual_score = compute_score("openai/gsm8k", response, actual_gt)
            print_result("æ¨¡å‹å›å¤è¯„åˆ†", True,
                        f"æå–ç­”æ¡ˆ: '{answer}', æ­£ç¡®ç­”æ¡ˆ: '{actual_gt}', å¾—åˆ†: {actual_score}")

    except Exception as e:
        print_result("Reward å‡½æ•°", False, str(e))
        all_passed = False

    # ============ æ£€æŸ¥ 5: rollout_loop.py ä¿®å¤ ============
    print_header("5. rollout_loop.py ä¿®å¤æ£€æŸ¥")
    try:
        with open('/p/scratch/westai0052/liu52/verl-agent/agent_system/multi_turn_rollout/rollout_loop.py', 'r') as f:
            content = f.read()

        fixed = 'isinstance(raw_prompt, (list, np.ndarray))' in content
        print_result("numpy array ç±»å‹æ£€æŸ¥å·²ä¿®å¤", fixed)

        if not fixed:
            all_passed = False
            print("       âš ï¸  éœ€è¦ä¿®å¤ rollout_loop.py ç¬¬ 98 è¡Œ")
            print("       å°† isinstance(raw_prompt, list) æ”¹ä¸º")
            print("       isinstance(raw_prompt, (list, np.ndarray))")

    except Exception as e:
        print_result("rollout_loop.py æ£€æŸ¥", False, str(e))

    # ============ æœ€ç»ˆç»“æœ ============
    print_header("æœ€ç»ˆç»“æœ")
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼è®­ç»ƒç®¡é“åº”è¯¥å¯ä»¥æ­£å¸¸å·¥ä½œã€‚")
        print("\nå¦‚æœè®­ç»ƒä»ç„¶å¤±è´¥ï¼Œå¯èƒ½çš„åŸå› æ˜¯ï¼š")
        print("  1. å­¦ä¹ ç‡å¤ªé«˜å¯¼è‡´ç­–ç•¥å´©æºƒ")
        print("  2. æ²¡æœ‰ KL æƒ©ç½šå¯¼è‡´æ¨¡å‹åç¦»å¤ªè¿œ")
        print("  3. Batch size å¤ªå°ï¼Œé‡‡æ ·ä¸åˆ°æ­£ç¡®ç­”æ¡ˆ")
    else:
        print("âŒ éƒ¨åˆ†æ£€æŸ¥å¤±è´¥ï¼Œè¯·å…ˆä¿®å¤ä¸Šè¿°é—®é¢˜å†è®­ç»ƒã€‚")

    return all_passed

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
